
## DNABERT
- [DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome](btab083.pdf)
- [URL](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680#446449819)

## DNABERT-2
- [DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome](https://arxiv.org/abs/2306.15006)
- [paper](2306.15006v2.pdf)

## Some useful sources of the original paper
- [The Linguistics of DNA](Searls-LinguisticsDNA-1992.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](1810.04805v2.pdf)
- [Promoter analysis and prediction in the human genome using sequence-based deep learning models](bty1068.pdf)
- [Neural Machine Translation of Rare Words with Subword Units](1508.07909v5.pdf): BPE Tokenization discussed in DNABERT2

### BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection
- [url](https://www.sciencedirect.com/science/article/pii/S1476927122001128)
- [paper](1-s2.0-S1476927122001128-main.pdf)