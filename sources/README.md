
## DNABERT
- [DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome](btab083.pdf)
- [URL](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680#446449819)

## DNABERT-2
- [DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome](https://arxiv.org/abs/2306.15006)
- [paper](2306.15006v2.pdf)

## Some useful sources of the original paper
- [The Linguistics of DNA](Searls-LinguisticsDNA-1992.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](1810.04805v2.pdf)
- [Promoter analysis and prediction in the human genome using sequence-based deep learning models](bty1068.pdf)