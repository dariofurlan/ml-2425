
## The paper on which we are focusing
- [DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome](btab083.pdf)
- [URL](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680#446449819)

## Some useful sources of the original paper
- [The Linguistics of DNA](Searls-LinguisticsDNA-1992.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](1810.04805v2.pdf)
- [Promoter analysis and prediction in the human genome using sequence-based deep learning models](bty1068.pdf)