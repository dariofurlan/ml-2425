\section{Experiments}

Building on the datasets described in the previous section, we conducted a series of experiments to evaluate the capabilities of DNABERT in the task of promoter prediction. Using the original 6-mer pretrained model released by the authors, we focused on the promoter prediction subsets from the GUE benchmark.

\subsection{Method}

We fine-tuned DNABERT separately on each subset. We repeated each experiment with at least three random seeds to test robustness. All training settings, including batch size and learning rate, were aligned with those reported in the original DNABERT paper. We used the official implementation as the base for all our experiments to ensure consistency with the reference results.

\subsection{Results}

Our results were evaluated using the \textit{Matthews Correlation Coefficient (MCC)}, consistent with the original paper. A summary of the best results obtained in our experiments is presented in \autoref{tab:mcc_results}, along with the expected values reported by the DNABERT authors. The comparison includes absolute and relative errors between our scores and the reference scores.

\begin{table}[h!]
    \centering
    \small % or \footnotesize
    \begin{tabular}{lccc}
        \toprule
        Dataset & Ours MCC & Reference MCC & Error \\
        \midrule
        all    & 90.65 & 90.48 & 0.17 [0.19] \\
        notata & 92.80 & 93.05 & 0.25 [0.27] \\
        tata   & 62.05 & 61.56 & 0.49 [0.80] \\
        \bottomrule
    \end{tabular}
    \caption{Average MCC scores (\%) versus reference values. Absolute error in percentage points; relative error in brackets.}
    \label{tab:mcc_results}
\end{table}

As shown in \autoref{tab:mcc_results}, the MCC scores match the published benchmarks within 0.5\% point, confirming that the model reproduces reliably.The small deviations observed are within acceptable margins and are likely due to the stochastic nature of training processes.

Repeating the fine-tuning with different random seeds is crucial for obtaining robust and reproducible results. While this practice is often implied, its significance becomes evident when aiming for a fair comparison with published benchmarks. In our experiments, averaging the outcomes across multiple runs helped mitigate the impact of random fluctuations, resulting in scores that faithfully mirror those in the original DNABERT study

\subsection{Run details}

To further support our findings, \autoref{tab:all_runs} reports the MCC scores for all individual runs, including the random seed used. These details reveal the extent of variability in the results and emphasize the importance of using multiple seeds when assessing model performance.

\begin{table}[h!]
    \centering
    \small % or \footnotesize
    \begin{tabular}{lcc}
        \toprule
        Dataset & Seed & MCC (\%) \\
        \midrule
        all    & 15  & 91.64 \\
        all    & 44  & 90.95 \\
        all    & 48  & 91.05 \\
        all    & 65  & 89.07 \\
        all    & 172 & 90.54 \\
        notata & 31  & 93.03 \\
        notata & 42  & 93.13 \\
        notata & 77  & 92.24 \\
        tata   & 11  & 63.76 \\
        tata   & 53  & 61.89 \\
        tata   & 80  & 60.48 \\
        \bottomrule
    \end{tabular}
    \caption{MCC by random seed for each dataset.}
    \label{tab:all_runs}
\end{table}

As visible in \autoref{tab:all_runs}, the \textit{tata} subset shows greater variability in MCC scores, likely due to its smaller size and the inherent difficulty of the classification task. Nevertheless, the overall results align closely with the published benchmarks.
