\section{Conclusion}

This project focused on reproducing the results of DNABERT and its successor DNABERT-2 for the task of promoter prediction in genomic sequences. By leveraging the datasets and methodologies described in the original papers, we successfully replicated the reported metrics, confirming the reliability and robustness of these models.

Through this process, we gained valuable insights into how language models, originally designed for natural language processing, can be adapted to genomic tasks. Specifically, the use of k-mer tokenization and self-attention mechanisms proved effective in capturing the complex patterns within DNA sequences. Our experiments also highlighted the importance of consistent training settings and the use of multiple random seeds to ensure reproducibility.

% While our scope was limited to reproducing the promoter prediction results, this exercise underscored the potential of Transformer-based models in genomics. By bridging the gap between machine learning and biology, these models offer promising tools for advancing our understanding of gene regulation and other genomic functions.

