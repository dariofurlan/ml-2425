{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d47bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "from typing import Dict, List, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AlbertConfig,\n",
    "    AlbertForSequenceClassification,\n",
    "    AlbertTokenizer,\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    BertForLongSequenceClassification,\n",
    "    BertForLongSequenceClassificationCat,\n",
    "    BertTokenizer,\n",
    "    DNATokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    FlaubertConfig,\n",
    "    FlaubertForSequenceClassification,\n",
    "    FlaubertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    XLMConfig,\n",
    "    XLMForSequenceClassification,\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaForSequenceClassification,\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMTokenizer,\n",
    "    XLNetConfig,\n",
    "    XLNetForSequenceClassification,\n",
    "    XLNetTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from transformers import glue_compute_metrics as compute_metrics\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = sum(\n",
    "    (\n",
    "        tuple(conf.pretrained_config_archive_map.keys())\n",
    "        for conf in (\n",
    "            BertConfig,\n",
    "            XLNetConfig,\n",
    "            XLMConfig,\n",
    "            RobertaConfig,\n",
    "            DistilBertConfig,\n",
    "            AlbertConfig,\n",
    "            XLMRobertaConfig,\n",
    "            FlaubertConfig,\n",
    "        )\n",
    "    ),\n",
    "    (),\n",
    ")\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"dna\": (BertConfig, BertForSequenceClassification, DNATokenizer),\n",
    "    \"dnalong\": (BertConfig, BertForLongSequenceClassification, DNATokenizer),\n",
    "    \"dnalongcat\": (BertConfig, BertForLongSequenceClassificationCat, DNATokenizer),\n",
    "    \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    \"xlnet\": (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    \"xlm\": (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "    \"albert\": (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer),\n",
    "    \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer),\n",
    "    \"flaubert\": (FlaubertConfig, FlaubertForSequenceClassification, FlaubertTokenizer),\n",
    "}\n",
    "                    \n",
    "TOKEN_ID_GROUP = [\"bert\", \"dnalong\", \"dnalongcat\", \"xlnet\", \"albert\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(args, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    pred_task_names = (args.task_name,)\n",
    "    pred_outputs_dirs = (args.predict_dir,)\n",
    "    if not os.path.exists(args.predict_dir):\n",
    "        os.makedirs(args.predict_dir)\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    predictions = {}\n",
    "    for pred_task, pred_output_dir in zip(pred_task_names, pred_outputs_dirs):\n",
    "        pred_dataset = load_and_cache_examples(args, pred_task, tokenizer, evaluate=True)\n",
    "\n",
    "        if not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(pred_output_dir)\n",
    "\n",
    "        args.pred_batch_size = args.per_gpu_pred_batch_size * max(1, args.n_gpu)\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        pred_sampler = SequentialSampler(pred_dataset)\n",
    "        pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=args.pred_batch_size)\n",
    "\n",
    "        # multi-gpu eval\n",
    "        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running prediction {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", len(pred_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.pred_batch_size)\n",
    "        pred_loss = 0.0\n",
    "        nb_pred_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "        for batch in tqdm(pred_dataloader, desc=\"Predicting\"):\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "                if args.model_type != \"distilbert\":\n",
    "                    inputs[\"token_type_ids\"] = (\n",
    "                        batch[2] if args.model_type in TOKEN_ID_GROUP else None\n",
    "                    )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "                outputs = model(**inputs)\n",
    "                _, logits = outputs[:2]\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        if args.output_mode == \"classification\":\n",
    "            if args.task_name[:3] == \"dna\" and args.task_name != \"dnasplice\":\n",
    "                probs = softmax(torch.tensor(preds, dtype=torch.float32))[:,1].numpy()\n",
    "            elif args.task_name == \"dnasplice\":\n",
    "                probs = softmax(torch.tensor(preds, dtype=torch.float32)).numpy()\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        elif args.output_mode == \"regression\":\n",
    "            preds = np.squeeze(preds)\n",
    "\n",
    "        result = compute_metrics(pred_task, preds, out_label_ids, probs)\n",
    "        \n",
    "        pred_output_dir = args.predict_dir\n",
    "        if not os.path.exists(pred_output_dir):\n",
    "               os.makedir(pred_output_dir)\n",
    "        output_pred_file = os.path.join(pred_output_dir, \"pred_results.npy\")\n",
    "        logger.info(\"***** Pred results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        np.save(output_pred_file, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d91db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    processor = processors[task]()\n",
    "    output_mode = output_modes[task]\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}_{}\".format(\n",
    "            \"dev\" if evaluate else \"train\",\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(args.max_seq_length),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "    if args.do_predict:\n",
    "        cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            \"dev\" if evaluate else \"train\",\n",
    "            str(args.max_seq_length),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\", \"xlmroberta\"]:\n",
    "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
    "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "        examples = (\n",
    "            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
    "        )   \n",
    "\n",
    "        \n",
    "        print(\"finish loading examples\")\n",
    "\n",
    "        # params for convert_examples_to_features\n",
    "        max_length = args.max_seq_length\n",
    "        pad_on_left = bool(args.model_type in [\"xlnet\"])\n",
    "        pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "        pad_token_segment_id = 4 if args.model_type in [\"xlnet\"] else 0\n",
    "\n",
    "\n",
    "        if args.n_process == 1:\n",
    "            features = convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            label_list=label_list,\n",
    "            max_length=max_length,\n",
    "            output_mode=output_mode,\n",
    "            pad_on_left=pad_on_left,  # pad on the left for xlnet\n",
    "            pad_token=pad_token,\n",
    "            pad_token_segment_id=pad_token_segment_id,)\n",
    "                \n",
    "        else:\n",
    "            n_proc = int(args.n_process)\n",
    "            if evaluate:\n",
    "                n_proc = max(int(n_proc/4),1)\n",
    "            print(\"number of processes for converting feature: \" + str(n_proc))\n",
    "            p = Pool(n_proc)\n",
    "            indexes = [0]\n",
    "            len_slice = int(len(examples)/n_proc)\n",
    "            for i in range(1, n_proc+1):\n",
    "                if i != n_proc:\n",
    "                    indexes.append(len_slice*(i))\n",
    "                else:\n",
    "                    indexes.append(len(examples))\n",
    "           \n",
    "            results = []\n",
    "            \n",
    "            for i in range(n_proc):\n",
    "                results.append(p.apply_async(convert_examples_to_features, args=(examples[indexes[i]:indexes[i+1]], tokenizer, max_length, None, label_list, output_mode, pad_on_left, pad_token, pad_token_segment_id, True,  )))\n",
    "                print(str(i+1) + ' processor started !')\n",
    "            \n",
    "            p.close()\n",
    "            p.join()\n",
    "\n",
    "            features = []\n",
    "            for result in results:\n",
    "                features.extend(result.get())\n",
    "                    \n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cfc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER = 6\n",
    "MODEL_PATH = f\"./ft/{KMER}/\"\n",
    "DATA_PATH = f\"sample_data/ft/{KMER}\"\n",
    "PREDICTION_PATH = f\"./result/{KMER}\"\n",
    "\n",
    "args = {\n",
    "    \"model_type\": \"dna\", # one of MODEL_CLASSES\n",
    "    \"tokenizer_name\": f\"dna{KMER}\",\n",
    "    \"model_name_or_path\": MODEL_PATH,\n",
    "    \"task_name\": \"dnaprom\",\n",
    "    \"do_predict\": True,\n",
    "    \"do_train\": False,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"max_seq_length\": 75,\n",
    "    \"per_gpu_pred_batch_size\": 128,\n",
    "    \"output_dir\": MODEL_PATH,\n",
    "    \"predict_dir\": PREDICTION_PATH,\n",
    "    \"cache_dir\": \"\",\n",
    "    \"n_process\": 2,\n",
    "    \"fp16\": False,\n",
    "    \"seed\": 42,\n",
    "    \"do_lower_case\": False,\n",
    "    \"overwrite_output_dir\": False,\n",
    "    \"overwrite_cache\": False,\n",
    "    \"config_name\": \"\",\n",
    "    \"no_cuda\": False\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "if (\n",
    "    os.path.exists(args.output_dir)\n",
    "    and os.listdir(args.output_dir)\n",
    "    and args.do_train\n",
    "    and not args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Prepare GLUE task\n",
    "args.task_name = args.task_name.lower()\n",
    "if args.task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "processor = processors[args.task_name]()\n",
    "args.output_mode = output_modes[args.task_name]\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=args.task_name,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "\n",
    "if args.model_type in [\"dnalong\", \"dnalongcat\"]:\n",
    "    assert args.max_seq_length % 512 == 0\n",
    "config.split = int(args.max_seq_length/512)\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "logger.info('finish loading model')\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Prediction\n",
    "tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "checkpoint = args.output_dir\n",
    "logger.info(\"Predict using the following checkpoint: %s\", checkpoint)\n",
    "prefix = ''\n",
    "model = model_class.from_pretrained(checkpoint)\n",
    "model.to(args.device)\n",
    "prediction = predict(args, model, tokenizer, prefix=prefix)\n",
    "    \n",
    "return prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
