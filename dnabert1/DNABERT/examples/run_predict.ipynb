{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d47bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/miniconda3/envs/dnabert/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    DNATokenizer,\n",
    ")\n",
    "from transformers import glue_compute_metrics as compute_metrics\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22c436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"dna\": (BertConfig, BertForSequenceClassification, DNATokenizer),\n",
    "    # \"dnalong\": (BertConfig, BertForLongSequenceClassification, DNATokenizer),\n",
    "    # \"dnalongcat\": (BertConfig, BertForLongSequenceClassificationCat, DNATokenizer),\n",
    "    # \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    # \"xlnet\": (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    # \"xlm\": (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    # \"roberta\": (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    # \"distilbert\": (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "    # \"albert\": (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer),\n",
    "    # \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer),\n",
    "    # \"flaubert\": (FlaubertConfig, FlaubertForSequenceClassification, FlaubertTokenizer),\n",
    "}\n",
    "                    \n",
    "TOKEN_ID_GROUP = [\"bert\", \"dnalong\", \"dnalongcat\", \"xlnet\", \"albert\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0c9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3729b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(args, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    pred_task_names = (args.task_name,)\n",
    "    pred_outputs_dirs = (args.predict_dir,)\n",
    "    if not os.path.exists(args.predict_dir):\n",
    "        os.makedirs(args.predict_dir)\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    predictions = {}\n",
    "    for pred_task, pred_output_dir in zip(pred_task_names, pred_outputs_dirs):\n",
    "        pred_dataset = load_and_cache_examples(args, pred_task, tokenizer, evaluate=True)\n",
    "\n",
    "        if not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(pred_output_dir)\n",
    "\n",
    "        args.pred_batch_size = args.per_gpu_pred_batch_size * max(1, args.n_gpu)\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        pred_sampler = SequentialSampler(pred_dataset)\n",
    "        pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=args.pred_batch_size)\n",
    "\n",
    "        # multi-gpu eval\n",
    "        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running prediction {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", len(pred_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.pred_batch_size)\n",
    "        pred_loss = 0.0\n",
    "        nb_pred_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "        for batch in tqdm(pred_dataloader, desc=\"Predicting\"):\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "                if args.model_type != \"distilbert\":\n",
    "                    inputs[\"token_type_ids\"] = (\n",
    "                        batch[2] if args.model_type in TOKEN_ID_GROUP else None\n",
    "                    )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "                outputs = model(**inputs)\n",
    "                _, logits = outputs[:2]\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        if args.output_mode == \"classification\":\n",
    "            if args.task_name[:3] == \"dna\" and args.task_name != \"dnasplice\":\n",
    "                probs = softmax(torch.tensor(preds, dtype=torch.float32))[:,1].numpy()\n",
    "            elif args.task_name == \"dnasplice\":\n",
    "                probs = softmax(torch.tensor(preds, dtype=torch.float32)).numpy()\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        elif args.output_mode == \"regression\":\n",
    "            preds = np.squeeze(preds)\n",
    "\n",
    "        result = compute_metrics(pred_task, preds, out_label_ids, probs)\n",
    "        \n",
    "        pred_output_dir = args.predict_dir\n",
    "        if not os.path.exists(pred_output_dir):\n",
    "               os.makedir(pred_output_dir)\n",
    "        output_pred_file = os.path.join(pred_output_dir, \"pred_results.npy\")\n",
    "        logger.info(\"***** Pred results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        np.save(output_pred_file, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d91db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    processor = processors[task]()\n",
    "    output_mode = output_modes[task]\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}_{}\".format(\n",
    "            \"dev\" if evaluate else \"train\",\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(args.max_seq_length),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "    if args.do_predict:\n",
    "        cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            \"dev\" if evaluate else \"train\",\n",
    "            str(args.max_seq_length),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\", \"xlmroberta\"]:\n",
    "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
    "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "        examples = (\n",
    "            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
    "        )   \n",
    "\n",
    "        \n",
    "        print(\"finish loading examples\")\n",
    "\n",
    "        # params for convert_examples_to_features\n",
    "        max_length = args.max_seq_length\n",
    "        pad_on_left = bool(args.model_type in [\"xlnet\"])\n",
    "        pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "        pad_token_segment_id = 4 if args.model_type in [\"xlnet\"] else 0\n",
    "\n",
    "\n",
    "        if args.n_process == 1:\n",
    "            features = convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            label_list=label_list,\n",
    "            max_length=max_length,\n",
    "            output_mode=output_mode,\n",
    "            pad_on_left=pad_on_left,  # pad on the left for xlnet\n",
    "            pad_token=pad_token,\n",
    "            pad_token_segment_id=pad_token_segment_id,)\n",
    "                \n",
    "        else:\n",
    "            n_proc = int(args.n_process)\n",
    "            if evaluate:\n",
    "                n_proc = max(int(n_proc/4),1)\n",
    "            print(\"number of processes for converting feature: \" + str(n_proc))\n",
    "            p = Pool(n_proc)\n",
    "            indexes = [0]\n",
    "            len_slice = int(len(examples)/n_proc)\n",
    "            for i in range(1, n_proc+1):\n",
    "                if i != n_proc:\n",
    "                    indexes.append(len_slice*(i))\n",
    "                else:\n",
    "                    indexes.append(len(examples))\n",
    "           \n",
    "            results = []\n",
    "            \n",
    "            for i in range(n_proc):\n",
    "                results.append(p.apply_async(convert_examples_to_features, args=(examples[indexes[i]:indexes[i+1]], tokenizer, max_length, None, label_list, output_mode, pad_on_left, pad_token, pad_token_segment_id, True,  )))\n",
    "                print(str(i+1) + ' processor started !')\n",
    "            \n",
    "            p.close()\n",
    "            p.join()\n",
    "\n",
    "            features = []\n",
    "            for result in results:\n",
    "                features.extend(result.get())\n",
    "                    \n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf7b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER = 6\n",
    "MODEL_PATH = f\"./ft/{KMER}/\"\n",
    "DATA_PATH = f\"sample_data/ft/{KMER}\"\n",
    "PREDICTION_PATH = f\"./result/{KMER}\"\n",
    "\n",
    "class AttrDict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = AttrDict({\n",
    "    \"model_type\": \"dna\", # one of MODEL_CLASSES\n",
    "    \"tokenizer_name\": f\"dna{KMER}\",\n",
    "    \"model_name_or_path\": MODEL_PATH,\n",
    "    \"task_name\": \"dnaprom\",\n",
    "    \"do_predict\": True,\n",
    "    \"do_train\": False,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"max_seq_length\": 75,\n",
    "    \"per_gpu_pred_batch_size\": 128,\n",
    "    \"output_dir\": MODEL_PATH,\n",
    "    \"predict_dir\": PREDICTION_PATH,\n",
    "    \"cache_dir\": \"\",\n",
    "    \"n_process\": 2,\n",
    "    \"fp16\": False,\n",
    "    \"seed\": 42,\n",
    "    \"do_lower_case\": False,\n",
    "    \"overwrite_output_dir\": False,\n",
    "    \"overwrite_cache\": False,\n",
    "    \"config_name\": \"\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01cfc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/19/2025 15:55:35 - WARNING - __main__ -   Process rank: None, device: cuda, n_gpu: 1, distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/uni/ml-2425/dnabert1/DNABERT/src/transformers/modeling_utils.py:651: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "finish loading examples\n",
      "number of processes for converting feature: 1\n",
      "1 processor started !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 8/8 [00:01<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    os.path.exists(args.output_dir)\n",
    "    and os.listdir(args.output_dir)\n",
    "    and args.do_train\n",
    "    and not args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Prepare GLUE task\n",
    "args.task_name = args.task_name.lower()\n",
    "if args.task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "processor = processors[args.task_name]()\n",
    "args.output_mode = output_modes[args.task_name]\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=args.task_name,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "\n",
    "if args.model_type in [\"dnalong\", \"dnalongcat\"]:\n",
    "    assert args.max_seq_length % 512 == 0\n",
    "config.split = int(args.max_seq_length/512)\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "logger.info('finish loading model')\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Prediction\n",
    "tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "checkpoint = args.output_dir\n",
    "logger.info(\"Predict using the following checkpoint: %s\", checkpoint)\n",
    "prefix = ''\n",
    "model = model_class.from_pretrained(checkpoint)\n",
    "model.to(args.device)\n",
    "prediction = predict(args, model, tokenizer, prefix=prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
