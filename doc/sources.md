
## DNABERT
- [DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome]([btab083.pdf](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680#446449819))

## DNABERT-2
- [DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome](https://arxiv.org/abs/2306.15006)

## Some useful sources of the original paper
- [The Linguistics of DNA](http://www.jstor.org/stable/29774782)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]([1810.04805v2.pdf](http://arxiv.org/abs/1810.04805))
- [Promoter analysis and prediction in the human genome using sequence-based deep learning models](https://doi.org/10.1093/bioinformatics/bty1068)
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909): BPE Tokenization discussed in DNABERT2

### BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection
- [BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection](https://www.sciencedirect.com/science/article/pii/S1476927122001128)
