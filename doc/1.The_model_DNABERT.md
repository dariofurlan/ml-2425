DNABERT is a model that follows the same paradigms introduced by BERT, but adapted to the DNA sequences.
BERT is a transformer-based contextualized language representation model that has achieved superhuman performance in many natural language processing (NLP) tasks. It introduces a paradigm of pre-training and fine-tuning, which first develops general-purpose understandings from massive amount of unlabeled data and then solves various applications with task-specific data with minimal architectural modification.

## Tokenization with k-mer
In order to apply a language model to DNA, it is necessary to break down the long genomic sequence of bases (A, T, C, G) into descrete tokens. DNABERT uses `k-mers`, which is all the subsequences of length k extracted with a step size of 1. For example, from "ATGGCT" we get the 3-mers (subsequences of length k): ATG, TGG, GGC, GCT.

The value of `k` influences the vocabulary size, resulting in 4^k possible combinations for k-mers. Each k-mer is encoded as a vector, which becomes the input to the model.

## Architecture

Il modello segue la stessa architettura di BERT-base:
	•	12 layer Transformer
	•	768 dimensioni per gli hidden states
	•	12 teste di attenzione

Durante l’addestramento, DNABERT usa una sequenza di massimo 512 token, compresa tra due token speciali: [CLS] e [SEP]. Come in BERT, il token [CLS] rappresenta l’intera sequenza e viene usato per le predizioni a livello globale (es. classificazione).

## Pretraining

L’addestramento iniziale (pretraining) è auto-supervisionato: non serve un dataset etichettato. Si maschera casualmente il 15% dei token (contigui, per evitare che il contesto immediato riveli la risposta) e si chiede al modello di predirli. L’obiettivo è imparare rappresentazioni generiche del DNA, basandosi sul contesto sia a monte che a valle della posizione di interesse.

Il pretraining di DNABERT è stato eseguito sul genoma umano completo e ha richiesto 25 giorni usando 8 GPU NVIDIA 2080Ti.

## Fine-tuning

Una volta pre-addestrato, DNABERT può essere facilmente adattato a compiti specifici come:
•	Predizione dei promotori
•	Identificazione di siti di splicing
•	Riconoscimento dei siti di legame per fattori di trascrizione

Basta aggiungere un classificatore finale e continuare l’addestramento su un piccolo dataset etichettato. Questo approccio permette buoni risultati anche quando i dati disponibili sono limitati.

2.5 Vantaggi

DNABERT ha introdotto tre vantaggi principali rispetto ai modelli precedenti (CNN, RNN):
	•	Cattura relazioni di lungo raggio grazie al meccanismo di attenzione globale
	•	Generalizza bene su più task
	•	Richiede pochi dati etichettati per il fine-tuning

Inoltre, la struttura stessa del modello permette una facile interpretazione dei segnali biologici tramite le mappe di attenzione.