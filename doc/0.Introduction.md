# Introduction

In recent years, researchers have started using language models—originally developed for natural language processing (NLP)—to study the genome. In this context, DNABERT is one of the first models that treats DNA as if it were a language, using a machine learning architecture called a Transformer. The idea is to represent DNA as a sequence of "words" made up of short nucleotide fragments called *k-mers*.

This project explores DNABERT and its updated version, DNABERT-2, focusing on a specific biological task: **promoter prediction**. Promoters are short regions in DNA that play a key role in starting the process of gene expression. Understanding where promoters are and how they work is important for studying how genes are turned on and off.

We will focus on the following aspects:
1. How DNABERT works and what makes it different from traditional models.
2. The main improvements introduced in DNABERT-2.
3. The promoter prediction task and why it matters.
4. A practical experiment to reproduce DNABERT's original promoter prediction results using Python.

By the end of this project, we aim to understand how Transformer-based models can be adapted for biology and what advantages they offer compared to earlier methods like convolutional or recurrent neural networks.

## DNA as a Language

DNA (deoxyribonucleic acid) is the molecule that contains the genetic instructions for how living organisms grow, function, and reproduce. It consists of two long strands twisted into a double helix. Each strand is made up of building blocks called *nucleotides*. Each nucleotide includes:
- A sugar molecule,
- A phosphate group,
- And one of four *bases*: adenine (A), thymine (T), cytosine (C), or guanine (G).

These bases are the alphabet of DNA. They pair up across the two strands (A with T, C with G) to form *base pairs*. The order of these bases carries the information cells use to build and maintain the body—just like letters form words and sentences in a language.

## Genes and Transcription

A *gene* is a segment of DNA that contains the instructions to make a specific protein or RNA molecule. Genes are responsible for most traits and functions in an organism.

The process of using a gene's instructions begins with **transcription**. During transcription, the DNA is copied into a molecule called *messenger RNA* (mRNA). This happens when an enzyme called *RNA polymerase* binds to a region at the beginning of the gene. This region is called a **promoter**.

After transcription, the mRNA leaves the cell nucleus and is used in **translation**, where it is read by ribosomes to build a protein. The sequence of bases in the mRNA determines the sequence of amino acids in the protein, which shapes its function.

## Promoters

A *promoter* is a short region of DNA located just before (upstream of) a gene. It acts like a starting signal for transcription. Promoters contain specific sequences that are recognized by RNA polymerase and other helper proteins called transcription factors. These sequences tell the machinery where to start copying the DNA into RNA.

Some promoters include a well-known sequence called the **TATA box**, which is usually found about 25 to 35 *base pairs* (i.e., steps along the DNA chain) before the gene's starting point. This box helps RNA polymerase find the right spot to begin transcription. However, not all promoters contain a TATA box. These are called *no-TATA promoters* and rely on other mechanisms to start transcription.

## Why Traditional Models Fall Short

Before DNABERT, most computational models used to analyze DNA sequences were based on **Convolutional Neural Networks (CNNs)** or **Recurrent Neural Networks (RNNs)**. These approaches made significant progress but also had clear limitations in this context.

CNNs are good at finding local patterns using sliding filters, but they only look at short windows of the sequence at a time. As a result, they often miss long-range relationships between distant parts of DNA—which are important in tasks like promoter recognition.

RNNs, including more advanced versions like LSTMs and GRUs, can handle longer sequences by processing them step-by-step. However, they struggle with long DNA sequences because earlier information fades away as the sequence progresses. This is known as the **vanishing gradient problem**. RNNs also compress all past information into a single internal state, which becomes a bottleneck for learning complex patterns.

In addition, both CNNs and RNNs usually need large amounts of labeled data to perform well. But in biology, high-quality labeled datasets are often limited and expensive to produce.

These challenges make it hard for traditional models to generalize well—especially when working with real-world genomic data. DNABERT addresses these problems by using self-attention, which allows it to consider the entire sequence at once and learn patterns across any distance.