## 5. The GUE Benchmark

As part of the DNABERT-2 project, the authors introduced a large benchmark called **GUE**, which stands for *Genome Understanding Evaluation*. This benchmark was designed to better test how well language models can understand and work with real genomic data.

GUE includes **28 classification tasks** based on DNA sequences from different species and experimental sources. These tasks cover a variety of biological signals, such as:
- Promoter regions,
- Transcription factor binding sites (TFBS),
- Enhancers,
- Splicing sites,
- And other regulatory elements.

Each task in GUE is treated as a binary classification problem: the model must decide whether a given DNA sequence contains the signal or not.

### Why GUE matters

Before GUE, most models—including DNABERT—were tested on small, hand-picked datasets, often from a single species (usually human). This made it hard to compare models fairly or evaluate how well they generalize across tasks and species.

GUE solves this by:
- Providing a **standardized evaluation** across many biological functions,
- Including **multi-species data** to test generalization,
- Covering both simple and challenging classification tasks.

The GUE benchmark was used in the DNABERT-2 paper to show that the new model performs better than previous models (like DNABERT, Enformer, and Nucleotide Transformer) in **23 out of 28 tasks**, while using fewer parameters and training resources.

### Scope of our project

In this project, we focus **only on the promoter prediction task** one of the classification tasks included in the GUE benchmark. This choice aligns with the original DNABERT paper and allows us to study a well-defined problem with strong biological relevance. Other tasks from GUE are outside the scope of our work.