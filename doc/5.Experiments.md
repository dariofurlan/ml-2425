## 6. Experiments

To test the capabilities of DNABERT on promoter prediction, we performed a series of experiments using the original **6-mer pretrained model** provided by the authors. We focused on the same datasets used in the DNABERT paper, which include promoter sequences from human DNA, divided into three subsets:
- **all**: the full dataset (TATA and no-TATA promoters combined),
- **notata**: only promoters without a TATA box,
- **tata**: only promoters that contain a TATA box.

### Method

We fine-tuned the model separately on each dataset. To ensure the results were not dependent on a specific random initialization, we repeated each experiment using different random seeds. This helped us confirm the consistency and reliability of the results.

All other training settings, such as batch size and learning rate, were kept consistent with those reported in the DNABERT paper. We used the official implementation as the basis for our experiments.

### Results

The following table shows a comparison between our best results and those reported in the original paper. We use **Matthews Correlation Coefficient (MCC)** as the evaluation metric, as in the original work.

| Dataset   | Our MCC   | Expected MCC   | Absolute Error   | Relative Error (%)   |
|:----------|:----------|:---------------|:-----------------|:---------------------|
| all       | 90.65%    | 90.48%         | 0.17%            | 0.19%                |
| notata    | 92.80%    | 93.05%         | 0.25%            | 0.27%                |
| tata      | 62.05%    | 61.56%         | 0.49%            | 0.80%                |

As shown above, our results are very close to the expected values. This confirms that the model reproduces well and that the official implementation is reliable. The small variations are within acceptable margins and likely due to randomness in initialization and training.

### Run Details

Below is the full list of runs with their associated random seeds and MCC scores:

| Dataset   | Seed   | MCC     |
|:----------|:------:|:--------|
| all       | 15     | 91.64%  |
| all       | 44     | 90.95%  |
| all       | 48     | 91.05%  |
| all       | 65     | 89.07%  |
| all       | 172    | 90.54%  |
| notata    | 31     | 93.03%  |
| notata    | 42     | 93.13%  |
| notata    | 77     | 92.24%  |
| tata      | 11     | 63.76%  |
| tata      | 53     | 61.89%  |
| tata      | 80     | 60.48%  |

These results highlight the variability due to randomness, especially in smaller datasets like *tata*, but overall show a consistent match with published benchmarks.

### Final Notes

It is worth noting that repeating the fine-tuning process with different random seeds is important to obtain stable and trustworthy results. While this may seem obvious, it becomes particularly relevant when comparing with published benchmarks. In our case, averaging across multiple runs helped smooth out the random variation and produced results that closely match those reported in the original DNABERT paper.